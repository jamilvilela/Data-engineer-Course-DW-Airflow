{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import csv\n",
    "import json\n",
    "import collections \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging config\n",
    "dt_now = datetime.now()\n",
    "year = dt_now.strftime('%Y')\n",
    "month = dt_now.strftime('%m')\n",
    "day = dt_now.strftime('%d')\n",
    "\n",
    "project_path = '/root/python3/Data-engineer-Course-DW-Airflow/logistics'\n",
    "data_path= f'{project_path}/data/'\n",
    "log_path = f'{project_path}/log/{year}-{month}-{day}-log.txt'\n",
    "map_path = f'{project_path}/data-map/map.json'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    filename=log_path,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filemode='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openFile(path: str):\n",
    "            map = ''\n",
    "            try:\n",
    "                  with open(path, 'r') as conf:\n",
    "                        map = json.loads(conf.read())\n",
    "                        logging.debug(f'Open file: {path}')\n",
    "                        \n",
    "            except FileNotFoundError:\n",
    "                  logging.error(f'File {path} not found.')    \n",
    "\n",
    "            return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotify(line, data_map):\n",
    "            \"\"\"\n",
    "            This function will place quotes before and after the values if it is a string type\n",
    "            \"\"\"\n",
    "\n",
    "            for field, value in line.items():\n",
    "\n",
    "                  if data_map['fields'][field] in ['string', 'datetime']:\n",
    "                        line[field] = f\"'{value}'\"\n",
    "            \n",
    "            return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(data_map: dict):\n",
    "            \"\"\"\n",
    "            This function read the CSV file for loading into DW.\n",
    "            If the column names are different from data mapping config, this function will return an empty list\n",
    "            \"\"\"\n",
    "             \n",
    "            file_name   = data_map['csv_file_name']\n",
    "            fields_name = data_map['fields'].keys()\n",
    "            data = []\n",
    "            \n",
    "            try:\n",
    "                  with open(f'{data_path}{file_name}', 'r') as file:\n",
    "                        reader = csv.DictReader(file)\n",
    "                        logging.info(f'Open file: {file_name}')\n",
    "            \n",
    "                        if collections.Counter(reader.fieldnames) != collections.Counter(fields_name):   \n",
    "                              logging.error('The file columns are different from data mapping. \\nIngestion process aborted.')\n",
    "                              return []\n",
    "\n",
    "                        for line in reader:\n",
    "                              line = quotify(dict(line), data_map)\n",
    "                              data.append(line)\n",
    "            \n",
    "            except FileNotFoundError:\n",
    "                  logging.error(f'File {file_name} not found.')\n",
    "                  return []\n",
    "                                    \n",
    "            logging.info(f'Total lines read: {len(data)}')\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_cmd(data: list, data_map: dict):\n",
    "            \"\"\"\n",
    "            this function receives the all csv file content in a list \n",
    "            and generates one sql command for each line of the file\n",
    "            \"\"\"                  \n",
    "\n",
    "            table       = data_map['table_name']\n",
    "            fields_name = data_map['fields'].keys()\n",
    "            fields_name = ','.join(fields_name)\n",
    "            unique_key  = data_map['unique_key']\n",
    "            sql_cmd     = ''\n",
    "            update_fields=''\n",
    "                        \n",
    "            for line in data:\n",
    "                  \n",
    "                  insert_values = [value for value in line.values()] \n",
    "                  insert_values = ','.join(insert_values)                 \n",
    "\n",
    "                  update_fields = [ f'{key}={value}' for key,value in line.items() if key not in unique_key ]\n",
    "                  update_fields = ','.join(update_fields)\n",
    "\n",
    "                  sql_cmd +=  f'''\n",
    "                              insert into lab6.{table} ({fields_name}) \n",
    "                              values ({insert_values}) \n",
    "                              on conflict({unique_key}) \n",
    "                              do update set {update_fields};\n",
    "                              ''' \n",
    "                                    \n",
    "            logging.info( f'Total commands: {len(data)}' )\n",
    "\n",
    "            return sql_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_postgres(sql_cmd: str):\n",
    "            \"\"\"\n",
    "            This function loads the data into Postgres using the SQL command.\n",
    "            \"\"\"\n",
    "            logging.debug( sql_cmd )\n",
    "\n",
    "            '''\n",
    "            load = PostgresOperator(task_id = 'load_to_postgres',\n",
    "                                    sql = sql_cmd,\n",
    "                                    postgres_conn_id = 'dw-postgresDB',\n",
    "                                    dag = dag_dw_load)\n",
    "\n",
    "            return load.execute()\n",
    "            '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upstram\n",
    "# loop reading all source files      \n",
    "file = openFile(map_path)\n",
    "file_map = file['params']\n",
    "\n",
    "logging.info('--------------------- Starting the DAG process ---------------------')\n",
    "      \n",
    "for file_number in range(8):\n",
    "    data_map = file_map[str(file_number)]\n",
    "    data     = read_csv(data_map)\n",
    "    sql_cmd  = create_sql_cmd(data, data_map)\n",
    "    load_to_postgres(sql_cmd)\n",
    "\n",
    "logging.info('----------------------- DAG process finished -----------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
